=== MODEL SETUP ===

preprocessing: True
classification: ternary
Twitter embeddings: True
Train set size: 50334
Test set size: 12284
Vocabulary size: 44350
earlystop | monitor: loss, min_delta: -0.01, patience: 2
epochs: 10
batch_size: 32
dropout: 0.5
extra train (ynacc): True
====================

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
main_input (InputLayer)      (None, 124)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 124, 300)          13305000  
_________________________________________________________________
dropout_1 (Dropout)          (None, 124, 300)          0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 124, 300)          721200    
_________________________________________________________________
lstm_2 (LSTM)                (None, 124, 300)          721200    
_________________________________________________________________
lstm_3 (LSTM)                (None, 300)               721200    
_________________________________________________________________
dense_1 (Dense)              (None, 100)               30100     
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 303       
=================================================================
Total params: 15,499,003
Trainable params: 2,194,003
Non-trainable params: 13,305,000
_________________________________________________________________
              precision    recall  f1-score   support

    negative      0.667     0.211     0.320      3972
     neutral      0.564     0.760     0.647      5937
    positive      0.464     0.590     0.519      2375

   micro avg      0.550     0.550     0.550     12284
   macro avg      0.565     0.520     0.496     12284
weighted avg      0.578     0.550     0.517     12284

[[ 837 2578  557]
 [ 360 4513 1064]
 [  58  916 1401]]
